{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akiitr/python/blob/master/fish_speech_tool.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Vv5Oa3jPMXX"
      },
      "source": [
        "# Fish Speech"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cloning the repo\n",
        "!git clone https://github.com/fishaudio/fish-speech\n",
        "\n",
        "#base linux pkgs\n",
        "!sudo apt-get install portaudio\n",
        "!sudo apt-get install gcc\n",
        "!sudo apt-get install portaudio19-dev python3-dev\n",
        "\n",
        "# python env buildding\n",
        "!pip install pip_tools\n",
        "!pip-compile fish-speech/pyproject.toml --output-file requirements.txt\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "#linux var\n",
        "import locale\n",
        "locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')\n",
        "\n",
        "# change the version based on the latest released current 1.5\n",
        "!huggingface-cli download fishaudio/fish-speech-1.5 --local-dir checkpoints/fish-speech-1.5/"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xtJ-VcpJPlVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# web gui (use public url as local does not work by passing app.launch(,share=True) in tools/run_webui.py)\n",
        "# use compile for the Nvidia CUDA usage\n",
        "# start the gui\n",
        "!python fish-speech/tools/run_webui.py \\\n",
        "    --llama-checkpoint-path checkpoints/fish-speech-1.5 \\\n",
        "    --decoder-checkpoint-path checkpoints/fish-speech-1.5/firefly-gan-vq-fsq-8x1024-21hz-generator.pth \\\n",
        "     --compile"
      ],
      "metadata": {
        "id": "RHWHv6CEn3NT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instruction from the repo:"
      ],
      "metadata": {
        "id": "GZE1pSNonrI8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVWiu3zLPMXZ"
      },
      "source": [
        "### For Windows User / win用户"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "bat"
        },
        "id": "JUan7RDmPMXZ"
      },
      "outputs": [],
      "source": [
        "!chcp 65001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W76qmhNSPMXa"
      },
      "source": [
        "### For Linux User / Linux 用户"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqrVmVhaPMXa"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaCru_zIPMXa"
      },
      "source": [
        "### Prepare Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5P7b3sPvPMXa"
      },
      "outputs": [],
      "source": [
        "# For Chinese users, you probably want to use mirror to accelerate downloading\n",
        "# !set HF_ENDPOINT=https://hf-mirror.com\n",
        "# !export HF_ENDPOINT=https://hf-mirror.com\n",
        "!cd fish-speech/\n",
        "!huggingface-cli download fishaudio/fish-speech-1.5 --local-dir checkpoints/fish-speech-1.5/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R66R07XJPMXa"
      },
      "source": [
        "## WebUI Inference\n",
        "\n",
        "> You can use --compile to fuse CUDA kernels for faster inference (10x)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        },
        "id": "jefYpUxEPMXa"
      },
      "outputs": [],
      "source": [
        "!python fish-speech/tools/run_webui.py \\\n",
        "    --llama-checkpoint-path checkpoints/fish-speech-1.5 \\\n",
        "    --decoder-checkpoint-path checkpoints/fish-speech-1.5/firefly-gan-vq-fsq-8x1024-21hz-generator.pth \\\n",
        "    # --compile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnVvgn33PMXa"
      },
      "source": [
        "## Break-down CLI Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBaN2YQGPMXa"
      },
      "source": [
        "### 1. Encode reference audio: / 从语音生成 prompt:\n",
        "\n",
        "You should get a `fake.npy` file.\n",
        "\n",
        "你应该能得到一个 `fake.npy` 文件."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        },
        "id": "UjRhior6PMXb"
      },
      "outputs": [],
      "source": [
        "## Enter the path to the audio file here\n",
        "src_audio = r\"D:\\PythonProject\\vo_hutao_draw_appear.wav\"\n",
        "\n",
        "!python tools/vqgan/inference.py \\\n",
        "    -i {src_audio} \\\n",
        "    --checkpoint-path \"checkpoints/fish-speech-1.4/firefly-gan-vq-fsq-8x1024-21hz-generator.pth\"\n",
        "\n",
        "from IPython.display import Audio, display\n",
        "audio = Audio(filename=\"fake.wav\")\n",
        "display(audio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIEUw_2GPMXb"
      },
      "source": [
        "### 2. Generate semantic tokens from text: / 从文本生成语义 token:\n",
        "\n",
        "> This command will create a codes_N file in the working directory, where N is an integer starting from 0.\n",
        "\n",
        "> You may want to use `--compile` to fuse CUDA kernels for faster inference (~30 tokens/second -> ~300 tokens/second).\n",
        "\n",
        "> 该命令会在工作目录下创建 codes_N 文件, 其中 N 是从 0 开始的整数.\n",
        "\n",
        "> 您可以使用 `--compile` 来融合 cuda 内核以实现更快的推理 (~30 tokens/秒 -> ~300 tokens/秒)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        },
        "id": "C4cE_tAwPMXb"
      },
      "outputs": [],
      "source": [
        "!python tools/llama/generate.py \\\n",
        "    --text \"hello world\" \\\n",
        "    --prompt-text \"The text corresponding to reference audio\" \\\n",
        "    --prompt-tokens \"fake.npy\" \\\n",
        "    --checkpoint-path \"checkpoints/fish-speech-1.4\" \\\n",
        "    --num-samples 2\n",
        "    # --compile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZczelpiiPMXb"
      },
      "source": [
        "### 3. Generate speech from semantic tokens: / 从语义 token 生成人声:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        },
        "id": "-jgvg7KsPMXb"
      },
      "outputs": [],
      "source": [
        "!python tools/vqgan/inference.py \\\n",
        "    -i \"codes_0.npy\" \\\n",
        "    --checkpoint-path \"checkpoints/fish-speech-1.4/firefly-gan-vq-fsq-8x1024-21hz-generator.pth\"\n",
        "\n",
        "from IPython.display import Audio, display\n",
        "audio = Audio(filename=\"fake.wav\")\n",
        "display(audio)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}